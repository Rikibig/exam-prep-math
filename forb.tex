\documentclass{article}

\input{preamble.tex}

%\synctex=1
\newcommand{\Rho}{\mathrm{P}}

\title{Eksamensnoter}
\author{Ricardt Riis}

\begin{document}

\maketitle

\textbf{Meningen med materialet er som disposition til mundtlig eksamen}

\tableofcontents

\section*{Introduktion}
I det følgende vil overskriften angive hvilket spørgsmål der svares på. Da det
er bedst at svare på begge spørgsmål "samtidigt" eller i det mindste have en
nogenlunde flydende overgang mellem de to spørgsmål, virker det gavnligst at
beskrive spørgsmålene, og dernæst under \textit{EN} overskrift besvare
spørgsmålene.\\
Bemærk desuden, at jeg kun har ringe forståelse af hvad Sætning og Definition
betyder i streng matematisk konstekst.

\begin{tcolorbox}
	\section{Funktioner}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for vektorfunktioner.
		\item Brug vektorfunktioner til at bestemme de afledte af de trigonometriske funktioner.
	\end{enumerate}
\end{tcolorbox}
Her kan man komme ind på følgende:
\begin{itemize}
	\item Funktioner tager et input, og laver et output.
	\item For vektorfunktioner gælder at deres signatur er
		\[
			\mathbb{R} \rightarrow \mathbb{R}^2.
		\] 
	\item Vektorfunktioner skrives
		\[
			\vec{v}(t).
		\] 
	\item $t$ kaldes normalt \texttt{parameterværdi}.
	\item Præsenteres en vektorfunktion grafisk, benyttes en banekurve.\\
		\textit{Bemærk}: En banekurve viser ikke alt ved en vektorfunktion. Fx.
		er banekurverne for $\vektor{t^3}{t^3}$ og $\vektor{t}{t}$ de samme.
	\item Den afledede af en vektorfunktion kan bestemmes således
		\[
			\vec{v}\,'(t) = \vektor{x(t)}{y(t)}' = \vektor{x(t)'}{y(t)'}
		\] 
		\textit{I det følgende vil afledte funktioner skrives således}
		\[
			\dot{\vec{v}}(t) = {\vektor{x(t)}{y(t)}}'
		\] 
	\item Den afledede beskriver retningsvektoren for tangenten til en bestemt
		parameterværdi.

	\item Den afledede kaldes også hastighedsvektoren
	\item Dermed kan farten bestemmes ved længden af hastighedsvektoren
		\[
			\text{fart}(\vec{v}) = \left|\dot{\vec{v}}\right|
		\] 
	\item Se mere viden om vektorer i afsnit om vektorer.
\end{itemize}

\begin{definition}{Cosinus, sinus og tangens}{}
	Cosinus, $\cos$, sinus, $\sin$, og tangens, $\tan$, defineres ud fra
	enhedscirklen, der har radius 1.
	\begin{center}
	\begin{tikzpicture}[scale=2.0]
		\coordinate (A) at (0, 0);
		\coordinate (B) at (0.8660254037844387, 0);
		\coordinate (C) at (0.8660254037844387, 0.5);
		\coordinate (E) at (1, 0.5773502691896257);
		\coordinate (D) at (1, 0);
		\draw (A) -- (B) node [midway, below] {$\cos{t}$};
		\draw (C) -- (B) node [midway, left] {$\sin{t}$};
		\draw (A) -> (C) node [midway, above] {$\vec{r}$};
		\draw[dotted, thick] (C) -- (E);
		\draw[dotted, thick] (D) -- (E) node [midway, right] {$\tan{t}$};
		\draw pic [draw] {angle = B--A--C};
		\node [above] at (0.30, -0.025) {$t$};

		\draw[->,thick] (-1.5,0)--(1.5,0) node[right]{$x$};
		\draw[->,thick] (0,-1.5)--(0,1.5) node[above]{$y$};
		\node [draw,circle through=(D)] at (A) {};
	\end{tikzpicture}
	\end{center}
\end{definition}

\begin{lemma}{Idiotformlen}{idformel}
	\( \cos^2{t} + \sin^2{t} = 1 \)	
\end{lemma}

\begin{definition}{}{}
	\[
		\vec{r}(t) = \vektor{\cos{t}}{\sin{t}}
	\] 
\end{definition}

\begin{theorem}{Afledte cosinus og sinus}{cossindiff}
	\begin{align*}
		\cos' &= -\sin\\
		\sin' &= \cos
	\end{align*}
\end{theorem}

\textbf{Bevis:}
Det ønskes at finde $\dot{\vec{r}}(t)$, da vi i så fald kan bevise sætning
\ref{th:cossindiff}.

\smallskip

Det vides om $\vec{r}$ at den bevæger sig langs enhedscirklens periferi. Dvs.
at for hver omgang $\vec{r}$ bevæger sig omkring periferien, har punktet som
stedvektoren $\vec{r}$ beskriver bevæget sig $2\pi$. Da perioden for $\vec{r}$
også er $2\pi$ ved vi om farten af $\vec{r}$
\[
	|\dot{\vec{r}}(t)| = 1.
\] 
\textit{Bemærk, at længden af $\vec{r}$ også er 1 (jf. lemma \ref{lm:idformel}).}\\
Det vides om tangenter til punkter på en cirkel står ret på stedvektoren til
punktet. Dermed gælder
\[
	\dot{\vec{r}} = \hat{\vec{r}}.
\]
Derfor
\begin{align*}
	\dot{\vec{r}} = {\vektor{\cos}{\sin}} &= \widehat{\vektor{\cos}{\sin}} = \hat{\vec{r}}\\
						\vektor{\cos'}{\sin'}	&= \vektor{-\sin}{\cos}.
\end{align*}
Da lighedstegnet gælder koordinatvis, er beviset gennemført.

\begin{lemma}{Tangens}{tan}
	\(
		\tan = \frac{\sin}{\cos}
	\) 
\end{lemma}

\begin{theorem}{Afledede tangens}{difftan}
	\(
		\tan' = \tan^2 + 1
	\) 
\end{theorem}

\textbf{Bevis:}
\begin{align*}
	\tan' &= \left(\frac{\sin}{\cos}\right)'\\
		  &= \left(\sin \cdot \frac{1}{\cos}\right)'\\
		  &= \cos \cdot \frac{1}{\cos} + \sin \cdot \left(\frac{1}{\cos}\right)'\\
		  &= 1 + \sin \cdot \left(-\frac{1}{\cos^2}\right) \cdot (-\sin)\\
		  &= 1 + \frac{\sin^2}{\cos^2}\\
		  &= \tan^2 + 1
\end{align*}

\begin{tcolorbox}
	\section{Funktioner}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for plus- og gangefølger.
		\item Bevis at to gangefølger altid giver anledning til en potenssammenhæng.
	\end{enumerate}
\end{tcolorbox}

\begin{eksempel*}{Hvordan ville jeg gøre?}{}\\
	Jeg ville forklare stoffet i følgende rækkefølge.
	\begin{itemize}
		\item Introducer hvad plusfølger og gangefølger er 
			(definition \ref{df:plusfølge} og \ref{df:gangefølge}).\\
			Gør herunder brug af figur \ref{plus_og_gange} %TODO
		\item Forklar sætning \ref{th:gangegange}, evt. ved brug af figur \ref{gangegange}
		\item Hvis der er mere tid til overs, bevis logaritmeregnereglerne i rækkefølgen
			sætning \ref{th:logswap}, \ref{th:logpotenstilgange} og \ref{th:loggangeplus}.
	\end{itemize}
\end{eksempel*}

\begin{theorem}{Logaritme laver gange om til plus}{loggangeplus}
	\[\log_A{B \cdot C} = \log_AB + \log_AC\]
\end{theorem}
\textbf{Bevis:}
\begin{align*}
	\log_A{B \cdot C} &= \log_AB + \log_AC \iff\\
\intertext{Der opløftes i A for at potensregneregler kan benyttes}
	A^{\log_A{B \cdot C}} &= A^{\log_AB + \log_AC}\\
\intertext{Føromtalte potensregneregel benyttes}
						  &= A^{\log_AB} \cdot A^{\log_AC} \iff\\
\intertext{$A^{\log_A}$ går ud}
	B \cdot C &= B \cdot C
\end{align*}

\begin{theorem}{Logaritmeregneregel}{logpotenstilgange}
	$\log_AB^C = C \cdot \log_AB$
\end{theorem}
\textbf{Bevis:}
Opløftning er det samme som gentagen gange
\begin{align*}
	\log_AB^C &= \log_A{\underbrace{B \cdot B \cdots B}_{C\text{ gange}}}\\
\intertext{Jævnfør sætning \ref{th:loggangeplus} så kan man skrive}
			  &= \underbrace{\log_A{B} + \log_A{B} + \cdots + \log_A{B}}_{C\text{ gange}}\\
\intertext{Da der lægges sammen C gange, og gange er gentaget plus, så gælder}
			  &= C \cdot \log_AB
\end{align*}

\begin{theorem}{Logaritmeregneregel}{logswap}
	$A^{\log_BC} = C^{\log_BA}$.
\end{theorem}
\textbf{Bevis:}
Reglen opskrives
\begin{align*}
	A^{\log_BC} &= C^{\log_BA} \iff\\
\intertext{Vi gør på en smart måde ingenting}
	B^{\log_B{A^{\log_BC}}} &= B^{\log_B{C^{\log_BA}}} \iff\\
\intertext{Ifølge sætning \ref{th:logpotenstilgange} så kan vi}
	B^{\log_BC \cdot \log_B{A}} &= B^{\log_BA \cdot \log_B{C}}
\end{align*}
Da der har været ensbetydende pile er beviset fuldført.

\begin{definition}{Plusfølge}{plusfølge}
	En plusfølge lægger altid et bestemt tal til for hvert skridt i følgen.
\end{definition}

\begin{definition}{Gangefølge}{gangefølge}
	En gangefølge gange altid et bestemt tal for hvert skridt i følgen.
\end{definition}

\begin{figure}[H]
	\centering
	\caption{Forklaring af definitionerne}
	\includegraphics[width=0.8\textwidth]{plus_og_gange}
	\label{plus_og_gange}
\end{figure}

\begin{theorem}{Gangefølge - Gangefølge}{gangegange}
	To gangefølger,
	\begin{center}
		$c\cdot \Rho x^n$ og $d\cdot \Rho y^n$,
	\end{center}
	giver altid anledning til en potenssammenhæng
	\[
		y = \frac{d}{c^{\log_{\Rho x}{\Rho y}}}\cdot x^{\log_{\Rho x}{\Rho y}}.
	\]
	\textit{Bemærk at $c^{\log_{\Rho x}{\Rho y}}$ er 1, hvis x-følgen starter ved 1.}
\end{theorem}

y ønskes at kunne blive fundet ud fra vilkårlig x. Til det formål skal vi kende
$n$.
\begin{align*}
	x &= c\cdot \Rho x^n\\
\intertext{Der divideres med c, log rho x tages}
	n &= \log_{\Rho x}{\frac{x}{c}}
\end{align*}

Denne $n$ kan indsættes i udtrykket for $y_n$, så enhver y kan findes.
\begin{align*}
	y &= d\cdot \Rho y^{\log_{\Rho x}{\frac{x}{c}}}\\
\intertext{Per sætning \ref{th:logpotenstilgange}}
	&= d\cdot \left(\frac{x}{c}\right)^{\log_{\Rho x}{\Rho y}}\\
\intertext{Potensregneregel}
	&= \frac{d}{c^{\log_{\Rho x}{\Rho y}}}\cdot x^{\log_{\Rho x}{\Rho y}}
\end{align*}

\begin{figure}[H]
	\centering
	\caption{En grafisk fremstilling af ovenstående}
	\includegraphics[width=0.8\textwidth]{gangegange}
	\label{gangegange}
\end{figure}

\begin{tcolorbox}
	\section{Funktioner}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for funktioner af to variable.
		\item Udled formlen for hældningen af regressionslinjen ved mindste kvadraters metode.
	\end{enumerate}
\end{tcolorbox}

\begin{eksempel*}{Hvordan ville jeg gøre?}
	Jeg ville
	\begin{enumerate}
		\item Først tale om funktioner (tegne maskinen), og være opmærksom på
			to input.
		\item Tale om definitionen på partiel afledning.
		\item Tale om gradienten og stationære punkter.
		\item Påbegynde beviset for sætning \ref{th:linreg2}, og stoppe ved
			indsigten om
			\[
				\overline{y} = a_{reg} \cdot \overline{x} + b_{reg}.
			\] 
		\item Bevise sætning \ref{th:propreg} og \ref{th:aregpoint}.
		\item Færdiggøre beviset for sætning \ref{th:linreg2}.
	\end{enumerate}
	Derudover ville jeg henlede opmærksomheden på at man også godt kunne bevise
	sætning \ref{th:linreg2} kun ved brug af funktioner af to variable.
\end{eksempel*}

\begin{definition}{Partiel afledning}{pderi}
	En funktion af to variable, $f(x, y)$ kan afledes partielt
	\[
		\frac{\partial}{\partial x} f(x, y) = \lim_{h\rightarrow0} \frac{f(x+h, y) - f(x, y)}{h}.
	\] 
	Man kan gøre det samme med y.
\end{definition}

\begin{eksempel*}{Partiel afledning}{}
	Givet funktionen
	\[
		f(x, y) = x^2 + y^3 + xy,
	\] 
	så vil den partielt afledede med hensyn til $x$ være
	\[
		\frac{\partial}{\partial x}f(x, y) = \frac{\partial}{\partial x} x^2 + y^3 + xy = 2x + y.
	\] 
\end{eksempel*}

\begin{definition}{Gradient}{grad}
	Gradienten til en funktion af to variable, noteret $\nabla$ (nabla), defineres ved
	\[
		\nabla f(x, y) = \vektor{\frac{\partial}{\partial x}f(x, y)}
						   {\frac{\partial}{\partial y}f(x, y)}.
	\] 
\end{definition}

\begin{definition}{Stationære punkter}{station}
	Et stationært punkt til en funktion $f(x, y)$ defineres ved et punkt 
	$(x_0, y_0)$ hvor $x_0$ og $y_0$ opfylder
	\[
		\nabla f(x_0, y_0) = \vec{0}.
	\] 
\end{definition}

\begin{theorem}{Proportionalitetsregression}{propreg}
	Den bedste hældning for en ret linje ved brug af mindste kvadraters metode
	gennem (0, 0) er givet ved
	\[
	    a_{reg} = \frac{\sum_{i=1}^n x_i \cdot y_i}{\sum_{i=1}^n x_i^2}
	\] 
	hvor $n$ er så mange punkter der laves regression på, og $x_i$ er x-koordinaten
	til det i-te punkt, og $y_i$ tilsvarende er y-koordinaten til det i-te punkt.
\end{theorem}

\textbf{Bevis:}
Først opskrives kvadratsummen for en given hældning
\[
	KS(a) = \sum_{i=1}^n (x_i \cdot a - y_i)^2.
\] 

Nu ønskes den optimale hældning $a_{reg}$ fundet, således at kvadratsummen bliver
mindst. Der differentieres og sættes lig nul.
\begin{align*}
	KS'(a_{reg}) &= 0\\
				 &= \left(\sum_{i=1}^n (x_i \cdot a_{reg} - y_i)^2\right)'\\
\intertext{Grundet ledvis differentiation}
				 &= \sum_{i=1}^n \left((x_i \cdot a_{reg} - y_i)^2\right)'\\
				 \intertext{Kædereglen}
				 &= \sum_{i=1}^n \left(2(x_i \cdot a_{reg} - y_i) \cdot x_i\right)\\
				 &= \sum_{i=1}^n \left(2(x_i^2 \cdot a_{reg} - x_i \cdot y_i)\right)\\
				 \intertext{Distributiv lov}
				 &= 2 \cdot \sum_{i=1}^n (x_i^2 \cdot a_{reg} - x_i \cdot y_i)\\
				 &= \sum_{i=1}^n (x_i^2 \cdot a_{reg} - x_i \cdot y_i)\\
				 \intertext{Kommutativ lov}
				 &= \sum_{i=1}^n x_i^2 \cdot a_{reg} - \sum_{i=1}^n x_i \cdot y_i\\
				 \intertext{Distributiv lov}
				 &= a_{reg} \cdot \sum_{i=1}^n x_i^2 - \sum_{i=1}^n x_i \cdot y_i\\
				 \intertext{$a_{reg}$ isoleres}
	     a_{reg} &=  \frac{\sum_{i=1}^n x_i \cdot y_i}{\sum_{i=1}^n x_i^2}.
\end{align*}

\begin{theorem}{Hældning for ret linje gennem givet punkt}{aregpoint}
	Den bedste hældning for en ret linje gennem $(x_0, y_0)$ er givet ved
	\[
		a_{reg} = \frac{\sum_{i=1}^n (x_i-x_0)(y_i-y_0)}{\sum_{i=1}^n (x_i-x_0)^2}
	\] 
	hvor $n$ er så mange punkter der laves regression på, og $x_i$ er x-koordinaten
	til det i-te punkt, og $y_i$ tilsvarende er y-koordinaten til det i-te punkt.
\end{theorem}

\textbf{Bevis:}
Koordinatsystemet forskydes således at punktet $(x_0, y_0)$ kommer til at ligge
i origo. Nu kan sætning \ref{th:propreg} benyttes. For at forskyde
koordinatsystemet trækkes $x_0$ fra x-koordinaten for hvert punkt og $y_0$ fra
y-koordinaten for hvert punkt.

\begin{theorem}{Lineær regression}{linreg2}
	Givet en liste af punkter (x, y), så kan man finde den bedste rette linje
	ved mindste kvadraters metode med følgende hældning $a_{reg}$ og
	begyndelsesværdi $b_{reg}$.
	\[
		a_{reg} = \frac{\sum_{i=1}^n (x_i-\overline{y})(y_i-\overline{y})}{\sum_{i=1}^n (x_i-\overline{x})^2}.
	\] 
	\[
		b_{reg} = \overline{y} - a_{reg} \cdot \overline{x}.
	\] 
	hvor $n$ er så mange punkter der laves regression på, og $x_i$ er x-koordinaten
	til det i-te punkt, og $y_i$ tilsvarende er y-koordinaten til det i-te punkt,
	$\overline{x}$ er gennemsnits x-koordinaten og $\overline{y}$ er
	gennemsnits y-koordinaten.
\end{theorem}

\textbf{Bevis:}
Kvadratsummen for den lineære regression opskrives
\[
	KS(a, b) = \sum_{i=1}^n (x_i \cdot a + b - y_i)^2.
\] 
Da det ønskes at finde minimum for funktionen, og et minimum er et stationært
punkt, og der kun er et stationært punkt for funktionen, som også er et
minimum, kan definition \ref{df:station} bruges.

\smallskip

If. definition \ref{df:grad}
\[
	\frac{\partial}{\partial a} KS(a_{reg}, b_{reg}) = 0 \land
	\frac{\partial}{\partial b} KS(a_{reg}, b_{reg}) = 0.
\] 
Ligningssystemet løses, først afledes kvadratsumsfunktion med hensyn til
begyndelsesværdien.
\begin{align*}
	0 &= \frac{\partial}{\partial b} KS(a_{reg}, b_{reg})\\
	  &= \frac{\partial}{\partial b} \sum_{i=1}^n (x_i \cdot a_{reg} + b_{reg} - y_i)^2\\
	  &= \sum_{i=1}^n 2 \cdot (x_i \cdot a_{reg} + b_{reg} - y_i)\\
	  &= 2 \cdot \sum_{i=1}^n (x_i \cdot a_{reg} + b_{reg} - y_i)\\
	  &= n \cdot b_{reg} + \sum_{i=1}^n (x_i \cdot a_{reg} - y_i)\\
	  &= b_{reg} + \sum_{i=1}^n (\frac{x_i}{n} \cdot a_{reg} - \frac{y_i}{n})\\
	  &= b_{reg} + (\overline{x} \cdot a_{reg} - \overline{y})\\
	  \intertext{Nu kan enten $a$ eller $b$ isoleres, og der kan differentieres
		  partielt med hensyn til den anden variabel, ligningssystemet kan
	  løses. I stedet indses følgende}
	\overline{y} &= a_{reg} \cdot \overline{x} + b_{reg}
\end{align*}
Dette er meget centralt, for det fortæller, at punktet $\overline{x}$ og
$\overline{y}$ ligger på regressionslinjen. Dermed kan hældningen findes ved
sætning \ref{th:aregpoint}
\[
	a_{reg} = \frac{\sum_{i=1}^n (x_i-\overline{y})(y_i-\overline{y})}{\sum_{i=1}^n (x_i-\overline{x})^2}.
\] 
Begyndelsesværdien er triviel at finde
\[
	b_{reg} = \overline{y} - a_{reg} \cdot \overline{x}.
\] 

\begin{tcolorbox}
	\section{Differential- og integralregning}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for diskret analyse.
		\item Bevis mindst en regneregel for diskret differentiation.
	\end{enumerate}
\end{tcolorbox}
\begin{itemize}
	\item I diskret analyse benyttes talfølger; i stedet for tal funktioner.
	\item For at gøre notationen af uendeligt lange talfølger nemmere, kan man benytte sig af funktionsnotation
		\[
			f(x), x\in\mathbb{N}.
		\]
		\begin{eksempel}{Indeksering}{}\\
			Hvis $f$ defineres ved talfølgen $1, 3, 5, 4, 9$, så vil
			\[
				f(3) = 5,
			\] 
			og $f(2.5)$ være udefineret.
		\end{eksempel}
	\item Der indføres desuden en anden potensfunktion
		\[
			x^{\overline{n}} = \frac{x!}{(x-n)!}
		\] 
		\begin{eksempel}{x i n-streg}{x-i-n-streg}
			$
				7^{\overline{4}} = 7 \cdot 6 \cdot 5 \cdot 4.
			$ 
		\end{eksempel}
	\item Det kaldes at differentiere når man trækker nabo-elementer fra hinanden
		\[
			\Delta f: [2, 2, -1, 5].
		\] 
	\item Det kan skrives med symboler
		\begin{definition}{Differentiation}{diff}
			$
				\Delta f(x) = f(x + 1) - f(x).
			$ 
		\end{definition}
	\item Man kan desuden definere integration
		\begin{definition}{Integration}{dint}
			Givet en talfølge defineret ved alle heltal i intervallet 
			$[a;b],\: a,b\in\mathbb{Z}$, så er alle stamfølger givet ved
			\[
				F(x_0) = \sum_a^{x_0} f(x) + k,
			\]
			hvor $k \in \mathbb{R}$.
		\end{definition}
	\item Det følger let at følgende gælder
		\begin{theorem}{Integralregningens hovedsætning}{main}
			\[
				\sum_a^b f(x) = F(b + 1) - F(a)
			\]
		\end{theorem}
\end{itemize}

\begin{definition}{Produkt af to følger}{listproduct}
	$ (f \cdot g)(x) = f(x) \cdot g(x) $ 
\end{definition}

\begin{theorem}{Produktregel for differentiation}{produktdiff}
	\[
		\Delta (f \cdot g)(x) = \Delta f(x) \cdot g(x) + f(x + 1) \cdot \Delta g(x)
	\] 
\end{theorem}
\textbf{Bevis:}
\begin{align*}
	\Delta (f \cdot g)(x) &= \Delta (f(x) \cdot g(x))\\
	\intertext{Ud fra definition af at gange to følger med hinanden}
						  &= f(x + 1) \cdot g(x + 1) - f(x) \cdot g(x)\\
						  &= f(x + 1) \cdot (g(x) + \Delta g(x)) - (f(x + 1) - \Delta f(x)) \cdot g(x)\\
						  &= f(x + 1) \cdot g(x) + f(x + 1) \cdot \Delta g(x) \\
						  &\gap- f(x + 1) \cdot g(x) + \Delta f(x) \cdot g(x)\\
						  &= \tcbhighmath{f(x + 1) \cdot g(x)} + f(x + 1) \cdot \Delta g(x) \\
						  &\gap\tcbhighmath{- f(x + 1) \cdot g(x)} + \Delta f(x) \cdot g(x)\\
						  &= f(x + 1) \cdot \Delta g(x) + \Delta f(x) \cdot g(x)
\end{align*}

\begin{theorem}{Differentiation af $x^{\overline{n}}$}{diffxn}
	Givet konstant $n$ og variabel $x$ gælder
	\[
		\Delta x^{\overline{n}} = n \cdot x^{\overline{n-1}}.
	\]
\end{theorem}
\textbf{Bevis:}
\begin{align*}
	\Delta x^{\overline{n}} &= (x+1)^{\overline{n}} - x^{\overline{n}}\\
	&= (x+1) \cdot x^{\overline{n-1}} - x^{\overline{n-1}} \cdot (x - (n-1))\\
\intertext{Det er vigtigt at indse hvorfor den sidste faktor er $(x - (n-1)$.
	\textit{Se eksempel \ref{ex:x-i-n-streg}.}}
	&= (x + 1 - x + (n - 1)) \cdot x^{\overline{n-1}}\\
	&= n \cdot x^{\overline{n-1}}
\end{align*}

\begin{tcolorbox}
	\section{Differential- og integralregning}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for
			differential- og integralregning		
		\item Bevis mindst en regneregel for differentiation	
	\end{enumerate}
\end{tcolorbox}

\begin{eksempel*}{Hvordan ville jeg gøre?}
	\begin{enumerate}
		\item Bevise sætning \ref{th:chainrule}, inddrage viden om kontinuitet
			og differentiabilitet,
		\item Bevise sætning \ref{th:xinte} ved brug af sætning
			\ref{th:difflog} og \ref{th:chainrule}
		\item Bevise sætning \ref{th:difflog}
	\end{enumerate}
\end{eksempel*}

\begin{definition}{Kontinuitet}{kont}
	Grænseværdien for $f(x)$ når $x$ går mod $x_0$ er $f(x_0)$
	\[\lim_{x\to x_0} f(x) = f(x_0).\]	
\end{definition}

For at noget skal være kontinuert, skal, hvis x-værdierne er tæt på hinanden,
så skal y-værdierne også være tæt på hinanden. Vi går ud fra, at alle
standardfunktioner er kontinuerte.

\newcommand\sek[2]{d_{#1,#2}}
\begin{definition}{Sekanthældning}{sekant}
	Sekanthældningen er hældningen for en ret linje gennem to forskellige
	punkter på en funktion $f$.
	\[
		d_{f,x_0}(x) = \frac{f(x) - f(x_0)}{x - x_0}
	\] 
\end{definition}

\begin{definition}{Differentiabilitet}{diffhed}
	Differentialkvotienten er grænseværdien for sekanthældningen for $f$ i
	x-værdierne $x$ og $x_0$ når $x$ går mod $x_0$
	\[f'(x_0) = \lim_{x \to x_0} d_f(x)\]
\end{definition}

\begin{theorem}{Kædereglen}{chainrule}
	\[
		(f \circ g)' = (f' \circ g) \cdot g'
	\] 
\end{theorem}

\textbf{Bevis:}
\begin{align*}
	d_{f \circ g,x_0}(x) &= \frac{(f \circ g)(x) - (f \circ g)(x_0)}{x - x_0}\\
					  &= \frac{f(g(x)) - f(g(x_0))}{x - x_0}\\
	\intertext{Der ganges med 1 på en sjov måde}
					  &= \frac{f(g(x)) - f(g(x_0))}{x - x_0} \cdot 
					  \frac{g(x) - g(x_0)}{g(x) - g(x_0)}\\
	\intertext{Vha. brøkregneregel}
					  &= \frac{f(g(x)) - f(g(x_0))}{g(x) - g(x_0)} \cdot 
					  \frac{g(x) - g(x_0)}{x - x_0}\\
					  &= d_{f,g(x_0)}(g(x)) \cdot d_{g,x_0}(x)
\end{align*}
Grundet viden om sekanthældninger og grænseværdier
\begin{align*}
	\lim_{x \to x_0} d_{f \circ g,x_0}(x) &= (f \circ g)'(x_0)\\
	 		&= \lim_{x \to x_0} d_{f,g(x_0)}(g(x)) \cdot d_{g,x_0}(x)\\
	\intertext{Når $x$ går mod $x_0$, og hvis $g$ er kontinuert og ikke
	konstant, så går $g(x)$ mod $g(x_0)$. Dermed}
	 		&= f'(g(x_0)) \cdot g'(x_0)
\end{align*}
Da $x_0$ er vilkårlig gælder
\[
	(f \circ g)' = (f' \circ g) \cdot g'.
\] 

\begin{theorem}{Afledte logaritme}{difflog}
	$(\ln \cdot)' = \frac{1}{\cdot}$
\end{theorem}

\textbf{Bevis:}
Med kædereglen
\begin{align*}
	e^{\ln x} &= x\\
	\left( e^{\ln x} \right)' &= x'\\
	x \cdot (\ln x)' &= 1\\
	(\ln x)' &= \frac{1}{x}
\end{align*}
Da $x$ er vilkårlig
\[
	(\ln \cdot )' = \frac{1}{\cdot}
\] 

\begin{theorem}{Afledte $\cdot^{n}$}{xinte}
	\[
		(\cdot^n)' = n \cdot ^{n-1}
	\] 
\end{theorem}

\textbf{Bevis:}
\begin{align*}
	(\ln{x^n})' &= (n \cdot \ln{x})'\\
	(\ln{x^n})' &= n (\ln{x})'\\
	\frac{1}{x^n} \cdot (x^n)'	&= n \cdot \frac{1}{x}\\
	(x^n)' &= x^n \cdot n \cdot \frac{1}{x}\\
		   &= n \cdot x^{n-1}
\end{align*}
Og da $x$ er vilkårlig
\[
	(\cdot^n)' = n \cdot ^{n-1}.
\] 

\begin{tcolorbox}
	\section{Differential- og integralregning}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for 
			differential- og integralregning
		\item Redegør for sammenhængen mellem stamfunktion og areal
	\end{enumerate}
\end{tcolorbox}
Se definitioner i tidligere afsnit.

\begin{theorem}{Stamfunktion}{stamfn}
	Lad $f$ være en kontinuert funktion defineret i intervallet $[a; b]$. Da er
	$f$s stamfunktion, $F$,
	\[
		F(x) = \int_a^x f(t) \dx.		
	\] 
\end{theorem}
% TODO: jeg er noget forvirret, for jeg ser hvad wikipedia mener, men vi har i
% klassen fået præsenteret det geometriske forståelse... Hvordan viser man
% dette bedst?

\textbf{Bevis:}
% TODO: forklar bedre, indsæt figur.
Antag, at $A(x)$ måler "arealet" under en graf fra $-\infty$ til $x$. Arealet
har her et fortegn, dvs. at hvis grafen er under nul, så tæller den negativt,
medens hvis grafen er over nul så tæller den positivt.

Ønsker man at finde arealet mellem to x-koordinater $x$ og $x_0$, kan dette
lade sig gøre ved
\[
	A(x) - A(x_0).
\] 
Dette udtryk kan approximeres, men der kommer en fejl, kaldet $E$
\[
	A(x) - A(x_0) = f(x_0) \cdot (x-x_0) + E.
\] 
Der divideres med $x-x_0$.
\begin{align*}
	\frac{A(x)-A(x_0)}{x-x_0} &= f(x_0) + \frac{E}{x-x_0}\iff\\
	\sek{A}{x_0}(x) &= f(x_0) + \frac{E}{x-x_0} \xlongrightarrow[x \to x_0]{}\\
	A'(x_0) &= \lim_{x \to x_0} f(x_0) + \frac{E}{x-x_0}
\end{align*}

Det undersøges nu, hvad der sker med $\frac{E}{x-x_0}$, når $x \to x_0$.
Vi ved om $E$
\[
	E \le (f(x_h) - f(x_l))(x-x_0),
\] 
hvor $x_h$ og $x_l$ noterer x-koordinaten til henholdsvis det højeste og
laveste punkt på grafen mellem $x$ og $x_0$. Men fordi $x_h$ og $x_l$ skal
ligge mellem $x$ og $x_0$, så vil både $x_h$ og $x_l$ gå mod $x_0$ når $x$ går
mod $x_0$. Med lidt omskrivninger fås altså
\begin{align*}
	\frac{E}{x-x_0} &\le f(x_h) - f(x_l) \xlongrightarrow[x \to x_0]{}\\
					&= f(x_0) - f(x-0)\\
					&= 0.
\end{align*}
Dvs.
\begin{align*}
	A'(x_0) &= \lim_{x \to x_0} f(x_0) + \frac{E}{x-x_0}\\
		   &= f(x_0) + 0,
\end{align*}
fordi $f(x_0)$ ikke påvirkes af $x$. Altså er $A$ en stamfunktion til $f$.

\begin{theorem}{Integralregningens hovedsætning}{mainruleint}
	Givet en funktion $f$ der er kontinuert
	\[
		\int_a^b f(x) \dx = F(b) - F(a),
	\]
	hvor $F$ er en stamfunktion til $f$.
\end{theorem}

\begin{tcolorbox}
	\section{Differensligninger}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for differensligninger.
		\item Udled en løsningsformel til en selvvalgt type differensligning
	\end{enumerate}
\end{tcolorbox}

\begin{theorem}{Sum af potenser}{sumpot}
	\[
		\sum_{x=a}^b g^x = \frac{g^{b+1} - g^a}{g-1}.
	\] 
\end{theorem}

\textbf{Bevis:}
$g^x$ er en følge der integreres, ganske som i diskret analyse. Dermed skal der
blot findes stamfølgen til $g^x$. Det er $\frac{1}{g-1} g^x$, udledningen
overlades til læseren.

Således gælder
\begin{align*}
	\sum_{x=a}^b g^x &= \frac{1}{g-1} g^{b+1} - \frac{1}{g-1} g^a\\
					 &= \frac{g^{b+1} - g^a}{g-1}
\end{align*}


\begin{theorem}{Løsningsformel for $\Delta f(x) = r \cdot f(x) + p$}{}
	\[
		\Delta f(x) = r\cdot  f(x)+p, f(0) = b 
		\implies f(x) = b \cdot (r + 1)^x + p \cdot \frac{(r+1)^x - 1}{r},
	\] 
	hvor $p, b \in \mathbb{R}$ og $r \ne 0$.
\end{theorem}

\textbf{Bevis:}
Der definieres følgende for at undgå skrivekrampe $g = r+1$.

\smallskip

Første 4 elementer i $f$ skrives op.
\begin{align*}
	f(0) &= b\\
	f(1) &= g \cdot b + p\\
	f(2) &= g^2 \cdot b + gp + p\\
	f(3) &= g^3 \cdot b + g^2p + gp + p
\end{align*}
Det indses dermed hurtigt at følgende gælder
\[
	f(x) = g^x \cdot b + \sum_{i=0}^{x-1} g^i \cdot p
\] 
Efter en omskrivning
\[
	f(x) = b \cdot g^x + p \cdot \sum_{i=0}^{x-1} g^i
\] 
Grundet sætning \ref{th:sumpot}
\begin{align*}
	f(x) &= b \cdot g^x + p \cdot \frac{g^x - 1}{g - 1} \\
	&= b \cdot (r + 1)^x + p \cdot \frac{(r+1)^x - 1}{r}.
\end{align*}

%\newcommand\vbinom[2]{\ensuremath{\binom{#1}{#2}}} % I fald man kan lide "almen" notation.
\newcommand\vbinom[2]{\ensuremath{\nobreak{K(#1,\,#2)}}} % I fald man ikke kan ...
\begin{tcolorbox}
	\section{Sandsynlighedsregning}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af selvvalgt del af teorien for
			sandsynlighedsregning med fokus på binomialfordelingen.
		\item Bevis formlen $\vbinom{n}{r} = \frac{n^{\overline{r}}}{r!}$ ved at løse
			differensligningen \[\vbinom{n+1}{r} = \vbinom{n}{r-1} + \vbinom{n}{r}.\]
	\end{enumerate}
\end{tcolorbox}

\begin{theorem}{Binomialkoefficient}{binkoeff}
	Givet to ikke negative hele tal, $n$ og $r$, hvor $r \le n$ gælder det at
	binomialkoefficienten $\vbinom{n}{r}$ er på formen
	\[
		\vbinom{n}{r} = \frac{n^{\overline{r}}}{r!}.
	\] 
	\vbinom{n}{r} udtales "\textit{n} vælg \textit{r}".
\end{theorem}

\textbf{Bevis:}
Følgende differensligning indføres
\[
	\vbinom{n+1}{r} = \vbinom{n}{r-1} + \vbinom{n}{r}.
\] 

Det viser sig at være smart at isolere \vbinom{n}{r-1}.
\begin{align*}
	\vbinom{n}{r-1} &= \vbinom{n+1}{r} - \vbinom{n}{r}\\
	\intertext{\vbinom{n}{r} kaldes for det $n$'te element i den $r$'te følge}
	f_{r-1}(n) &= f_{r}(n+1) - f_{r}(n)\\
	\intertext{Idé fra diskret analyse indføres}
	f_{r-1}(n) &= \Delta f_{r}(n)
\end{align*}

Det er et super centralt resultat, for nu kan alle $f_{r}$ findes. Det skyldes
at $f_0(n)$ er kendt, og en hvilken som helst $f_r(n)$ kan findes ved at
integrere diskret $r$ gange med pluskonstant 0, og evaluere i $n$. Bevis ved
induktion.

\smallskip

\textbf{Startbetingelse:} $f_0(n) = 1$, da \vbinom{n}{0} er 1, fordi der kun kan
vælges nul elementer på en måde.

\textbf{Induktionsskridt:} Under antagelse af at $f_r(n) =
\frac{n^{\overline{r}}}{r!}$, vis da at $f_{r+1}(n)
=\frac{n^{\overline{r+1}}}{(r+1)!}$.

Idéen fra diskret analyse bruges nu
\begin{align*}
	f_{r}(n) 
	   &= \frac{n^{\overline{r}}}{r!}\\
	   &= \frac{(r+1) \cdot n^{\overline{r}}}{(r+1)!}\\
	   &= \frac{\Delta n^{\overline{r+1}}}{(r+1)!}\\
	   &= \Delta\frac{n^{\overline{r+1}}}{(r+1)!}\\
	   &= \Delta f_{r+1}(n)
\end{align*}
Det vil sige
\begin{align*}
	\Delta f_{r+1}(n) &= \Delta\frac{n^{\overline{r+1}}}{(r+1)!}\\
	f_{r+1}(n) &= \frac{n^{\overline{r+1}}}{(r+1)!}
\end{align*}
Dermed er induktionsbeviset afsluttet.

\smallskip

Da det nu er vist at $f_r(n) = \frac{n^{\overline{r}}}{r!}$, og $f_r(n) =
\vbinom{n}{r}$, da er differensligningen løst
\[
	\vbinom{n}{r} = \frac{n^{\overline{r}}}{r!}.
\] 

\hrulefill

\begin{itemize}
	\item Tæthedsfunktioner
	\item Bernoulli
	\item Binomialfordeling: Mange bernoulli
	\item K(n,r)
		\[
			K(n + 1, r) = K(n, r-1) + K(n, r)
		\]
\end{itemize}

\section{Statistik}
\subsection{Giv en præsentation af en selvvalgt del af teorien for hypotesetest}
\begin{itemize}
	\item Nulhypotese
	\item Hvornår er hvad rigtigt?
	\item Bevis
		\[
			P(p\in[\hat p \pm 2\cdot\sqrt{\frac{\hat p\cdot(1-\hat p)}{n}}]) \approx 95\%
		\]
	\item Ved $\hat p = \frac{X}{n}$, og $X \in $
\end{itemize}

\section{Transformationer}
\begin{itemize}
	\item Flytning (x, y)
	\item Strækning (x, y)
	\item $e^{-\frac{1}{2}x^2}$
	\item Tæthedsfunktioner
	\item Areal
	\item Skaler langs y.
	\item Strækning langs x, konsekvenser
	\item Middelværdi
\end{itemize}

\begin{tcolorbox}
	\section{Vektorer}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for vektorer.
		\item Gør rede for skalarproduktet og måder at beregne dette på.
	\end{enumerate}
\end{tcolorbox}

\begin{itemize}
	\item Vektorer er abstrakte pile, man kan lægge dem sammen og trække fra.
		Vektoren starter ikke et bestemt sted
	\item Består af en længde og en retning
\end{itemize}
\begin{theorem}{Skalarprodukt ved længder}{scalar_prod_len_vec}
	En regneoperation der ganger to vektorer sammen og giver et tal, som givet
	tre vektorer af samme dimension $\vec{a}$, $\vec{b}$ og $\vec{c}$ opfylder
	\begin{enumerate}
		\item $\vec{c} \cdot (\vec{a} + \vec{b}) = \vec{c} \cdot \vec{a} + \vec{c} \cdot \vec{b}$,
		\item $\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$, og
		\item $\vec{a}\,^2 = |\vec{a}|^2$,
	\end{enumerate}
	kan kun give
	\[
		\vec{a} \cdot \vec{b} = \frac{|\vec{a} + \vec{b}|^2 - |\vec{a}|^2 - |\vec{b}|^2}{2}.
	\] 
	\textit{Operationen kaldes også prikprodukt}.
\end{theorem}
\textbf{Bevis:}
\begin{align*}
	(\vec{a} + \vec{b})^2 &= (\vec{a} + \vec{b}) \cdot (\vec{a} + \vec{b})\\
						  &= (\vec{a} + \vec{b}) \cdot \vec{a} + (\vec{a} + \vec{b}) \cdot \vec{b}\\
						  &= \vec{a} \cdot (\vec{a} + \vec{b}) + \vec{b} \cdot (\vec{a} + \vec{b})\\
						  &= \vec{a}\,^2 + \vec{a} \cdot \vec{b} + \vec{b} \cdot \vec{a} + \vec{b}\,^2\\
						  &= \vec{a}\,^2 + 2 \cdot \vec{a} \cdot \vec{b} + \vec{b}\,^2\\
						  \intertext{Der omarrangeres}
	\vec{a} \cdot \vec{b} &= \frac{(\vec{a} + \vec{b})^2 - \vec{a}\,^2 - \vec{b}\,^2}{2}\\
						  &= \frac{|\vec{a} + \vec{b}|^2 - |\vec{a}|^2 - |\vec{b}|^2}{2}
\end{align*}

\begin{theorem}{Skalarprodukt ved dekomposanering af vektorer}{}
	Givet to vektorer $\vec{a}$ og $\vec{b}$, som kan dekomposaneres til $\vektor{a_1}{a_2}$ og $\vektor{b_1}{b_2}$, så gælder
	\[
		\vec{a} \cdot \vec{b} = a_1 \cdot b_1 + a_2 \cdot b_2.
	\] 
\end{theorem}
\textbf{Bevis:}
\begin{align*}
	\vec{a} \cdot \vec{b} &= \frac{|\vec{a} + \vec{b}|^2 - |\vec{a}|^2 - |\vec{b}|^2}{2}\\
						  &= \frac{(\vec{a} + \vec{b})_1^2 + (\vec{a} + \vec{b})_2^2 - (\vec{a}_1^2 + \vec{a}_2^2) - (\vec{b}_1^2 + \vec{b}_2^2)}{2}\\
						  &= \frac{(\vec{a}_1 + \vec{b}_1)^2 + (\vec{a}_2 + \vec{b}_2)^2 - (\vec{a}_1^2 + \vec{a}_2^2) - (\vec{b}_1^2 + \vec{b}_2^2)}{2}\\
						  &= \frac{\vec{a}_1^2 + 2 \cdot \vec{a}_1 \cdot \vec{b}_1 + \vec{b}_1^2 + \vec{a}_2^2 + 2 \cdot \vec{a}_2 \cdot \vec{b}_2 + \vec{b}_2^2 - (\vec{a}_1^2 + \vec{a}_2^2) - (\vec{b}_1^2 + \vec{b}_2^2)}{2}\\
						  &= \frac{2 \cdot \vec{a}_1 \cdot \vec{b}_1 + 2 \cdot \vec{a}_2 \cdot \vec{b}_2}{2}\\
						  &= \vec{a}_1 \cdot \vec{b}_1 + \vec{a}_2 \cdot \vec{b}_2
\end{align*}

\begin{theorem}{Skalarprodukt ved vinkel}{}
	Givet to vektorer $\vec{a}$ og $\vec{b}$, og vinklen mellem disse $v$, gælder
	\[
		\vec{a} \cdot \vec{b} = |\vec{a}| \cdot |\vec{b}| \cdot \cos{v}.
	\] 
\end{theorem}
\textbf{Bevis:}
Det er givet, at cosinusrelationen gælder
\[
	c^2 = a^2 + b^2 - 2 \cdot a \cdot b \cdot \cos{\gamma}.
\] 
\shorthandoff{"}
\begin{center}
\begin{tikzpicture}
	\coordinate (A) at (0, 0);
	\coordinate (B) at (3, 0);
	\coordinate (C) at (5, 2);
	\draw (A) -- (B) node [midway, below=1] {a};
	\draw (C) -- (B) node [midway, below=1] {b};
	\draw (A) -- (C) node [midway, above=1] {c};
 	\draw pic ["$\gamma$", draw] {angle = C--B--A};
\end{tikzpicture}
\end{center}

Der konstrueres en trekant så $c^2$ er $|\vec{a} + \vec{b}|^2$ og $a^2$ er
$|\vec{a}|^2$ og $b^2$ er $|\vec{b}^2|$.
\smallskip

\begin{center}
\begin{tikzpicture}
	\coordinate (A) at (0, 0);
	\coordinate (B) at (3, 0);
	\coordinate (C) at (5, 2);
	\draw[->] (A) -> (B) node [midway, below=1] {$\vec{a}$};
	\draw[->] (B) -> (C) node [midway, below=1] {$\vec{b}$};
	\draw[] (A) -> (C) node [midway, above=1] {$\vec{a} + \vec{b}$};
	\draw[dashed] (B) -- (5,0) coordinate (D);
 	\draw pic ["$\gamma$", draw] {angle = C--B--A};
 	\draw pic ["$v$", draw] {angle = D--B--C};
\end{tikzpicture}
\end{center}
\shorthandon{"}

I denne trekant vil $\gamma$ være givet ved $180 - v$. Dvs. at udtrykket for
$|\vec{a} + \vec{b}|^2$ kan indsættes i sætning \ref{th:scalar_prod_len_vec}.
Da fås
\begin{align*}
	\vec{a} \cdot \vec{b} &= \frac{|\vec{a}|^2 + |\vec{b}|^2 - 2 \cdot |\vec{a}| \cdot |\vec{b}| \cdot \cos{(180 - v)} - |\vec{a}|^2 - |\vec{b}|^2}{2}\\
						  &= \frac{- 2 \cdot |\vec{a}| \cdot |\vec{b}| \cdot \cos{(180 - v)}}{2}\\
						  &= -|\vec{a}| \cdot |\vec{b}| \cdot \cos{(180 - v)}\\
						  &= |\vec{a}| \cdot |\vec{b}| \cdot \cos{v}.
\end{align*}

\begin{tcolorbox}
	\section{Vektorer}
	\tcblower
	\begin{enumerate}
		\item Giv en præsentation af en selvvalgt del af teorien for vektorer.
		\item Bevis projektionsformlen og gør rede for, hvordan den kan bruges
			i forbindelse med regression eller afstand mellem punkt og linje.
	\end{enumerate}
\end{tcolorbox}

\begin{itemize}
	\item Kør ovenstående
	\item Bevis projektionsformlen
	\item Afstand mellem punkt og linje (mums)
	\item Regression hvis der er tid
\end{itemize}

\section{Annuiteter}
\begin{itemize}
	\item Tænkning kræves
\end{itemize}

\end{document}
